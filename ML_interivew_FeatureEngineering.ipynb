{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFzBWVIZBxxhyis44PLlMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/random_thoughts/blob/main/ML_interivew_FeatureEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yS1OjsBIoAP2"
      },
      "outputs": [],
      "source": [
        "# 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Feature hashing"
      ],
      "metadata": {
        "id": "h2z37CPWEN6X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example data\n",
        "data = [\n",
        "    {'feature1': 'cat', 'feature2': 'red'},\n",
        "    {'feature1': 'dog', 'feature2': 'blue'},\n",
        "    {'feature1': 'bird', 'feature2': 'green'},\n",
        "]\n",
        "\n",
        "# Define the hash function\n",
        "def feature_hashing(text, num_buckets):\n",
        "    return hash(text) % num_buckets\n",
        "\n",
        "# Get unique feature values\n",
        "feature1_values = set(sample['feature1'] for sample in data)\n",
        "feature2_values = set(sample['feature2'] for sample in data)\n",
        "\n",
        "# Define the number of hash buckets\n",
        "num_buckets = 5\n",
        "\n",
        "# Create a mapping of feature values to hash buckets\n",
        "feature1_hash_map = {value: feature_hashing(value, num_buckets) for value in feature1_values}\n",
        "feature2_hash_map = {value: feature_hashing(value, num_buckets) for value in feature2_values}\n",
        "\n",
        "# Convert the data into hashed features\n",
        "hashed_data = [\n",
        "    {'feature1': feature1_hash_map[sample['feature1']], 'feature2': feature2_hash_map[sample['feature2']]}\n",
        "    for sample in data\n",
        "]\n",
        "\n",
        "# Convert the data into PyTorch tensors\n",
        "feature1_tensor = torch.tensor([sample['feature1'] for sample in hashed_data], dtype=torch.long)\n",
        "feature2_tensor = torch.tensor([sample['feature2'] for sample in hashed_data], dtype=torch.long)\n",
        "\n",
        "# Create a tensor of indices for the EmbeddingBag layer\n",
        "indices = torch.tensor([0, 1, 2])  # Assuming three samples in the data\n",
        "\n",
        "# Concatenate the tensors along the second dimension\n",
        "input_tensor = torch.cat([feature1_tensor.unsqueeze(1), feature2_tensor.unsqueeze(1)], dim=1)\n",
        "\n",
        "# Define the embedding layer\n",
        "embedding_layer = nn.EmbeddingBag(num_buckets, embedding_dim=5, sparse=True)\n",
        "\n",
        "# # Forward pass\n",
        "# output = embedding_layer(input_tensor, indices)\n",
        "\n",
        "# Print the result\n",
        "print(\"Input Tensor:\")\n",
        "print(input_tensor)\n",
        "# print(\"\\nOutput Tensor:\")\n",
        "# print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZIW9peoERTL",
        "outputId": "8923b08c-9ee7-41e0-ed7c-dc050c36815d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor:\n",
            "tensor([[4, 1],\n",
            "        [4, 1],\n",
            "        [1, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PLjyoINEfdF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized Cross Entropy"
      ],
      "metadata": {
        "id": "qfkqloqfIZF_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NormalizedCrossEntropyLoss(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(NormalizedCrossEntropyLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        :param logits: Raw scores from the model (before softmax)\n",
        "        :param targets: Ground truth class indices\n",
        "        :return: Normalized Cross Entropy loss\n",
        "        \"\"\"\n",
        "        # Apply softmax to the logits\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Create a one-hot encoding of the targets\n",
        "        one_hot_targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
        "\n",
        "        # Calculate the log probabilities for the true classes\n",
        "        log_probabilities = torch.log(probabilities + 1e-10)  # Adding a small epsilon to avoid numerical instability\n",
        "\n",
        "        # Calculate the cross-entropy loss\n",
        "        cross_entropy_loss = -torch.sum(log_probabilities * one_hot_targets, dim=1)\n",
        "\n",
        "        # Calculate the normalization term\n",
        "        normalization_term = -torch.sum(one_hot_targets * log_probabilities, dim=1)\n",
        "\n",
        "        # Normalize the cross-entropy loss\n",
        "        normalized_cross_entropy_loss = cross_entropy_loss / normalization_term\n",
        "\n",
        "        # Take the mean over the batch\n",
        "        mean_loss = torch.mean(normalized_cross_entropy_loss)\n",
        "\n",
        "        return mean_loss\n",
        "\n",
        "# Example usage:\n",
        "num_classes = 5\n",
        "logits = torch.randn(32, num_classes)  # Assuming a batch size of 32\n",
        "targets = torch.randint(0, num_classes, (32,))\n",
        "\n",
        "nce_loss = NormalizedCrossEntropyLoss(num_classes)\n",
        "loss = nce_loss(logits, targets)\n",
        "\n",
        "print(\"Normalized Cross Entropy Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW4vtuMrQ6rB",
        "outputId": "5843539f-d67c-4e08-ce86-09a872267d0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Cross Entropy Loss: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Q2noZbIQ9Hv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}