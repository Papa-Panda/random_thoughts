{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+qIAt4GCcFQVW8AgqobQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/random_thoughts/blob/main/ML_interview_two_tower_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kglrMnF5OE1W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, user_dim, item_dim, embedding_dim):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "\n",
        "        # User tower\n",
        "        self.user_embedding = nn.Embedding(user_dim, embedding_dim)\n",
        "\n",
        "        # Item tower\n",
        "        self.item_embedding = nn.Embedding(item_dim, embedding_dim)\n",
        "\n",
        "        # Fully connected layers for interaction\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.output_layer = nn.Linear(64, 1)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, user_input, item_input):\n",
        "        # User tower\n",
        "        user_embedded = self.user_embedding(user_input)\n",
        "        user_embedded = user_embedded.view(user_embedded.size(0), -1)  # Flatten the user embeddings\n",
        "\n",
        "        # Item tower\n",
        "        item_embedded = self.item_embedding(item_input)\n",
        "        item_embedded = item_embedded.view(item_embedded.size(0), -1)  # Flatten the item embeddings\n",
        "\n",
        "        # Concatenate user and item embeddings\n",
        "        merged = torch.cat((user_embedded, item_embedded), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.relu(self.fc1(merged))\n",
        "        x = self.relu(self.fc2(x))\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "        return output.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Sample data\n",
        "user_data = torch.tensor([1, 2, 3, 4, 5])  # User IDs\n",
        "item_data = torch.tensor([101, 102, 103, 104, 105])  # Item IDs\n",
        "labels = torch.tensor([5.0, 4.0, 3.0, 4.5, 2.5])  # Ratings\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 10\n",
        "user_dim = max(user_data) + 1\n",
        "item_dim = max(item_data) + 1\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Create model, loss function, and optimizer\n",
        "model = TwoTowerModel(user_dim, item_dim, embedding_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(user_data, item_data, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for batch_user, batch_item, batch_labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_user, batch_item)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Inference\n",
        "user_input = torch.tensor([2])  # User ID for inference\n",
        "item_input = torch.tensor([103])  # Item ID for inference\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model(user_input, item_input)\n",
        "\n",
        "print(f\"Prediction for User {user_input.item()} on Item {item_input.item()}: {prediction.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT8UTYsgOMCJ",
        "outputId": "4cd5ae0e-a368-440d-9d36-85c53aa12a4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 25.113325119018555\n",
            "Epoch 2/100, Loss: 7.805745601654053\n",
            "Epoch 3/100, Loss: 7.23158073425293\n",
            "Epoch 4/100, Loss: 4.651462554931641\n",
            "Epoch 5/100, Loss: 5.96266508102417\n",
            "Epoch 6/100, Loss: 3.467662811279297\n",
            "Epoch 7/100, Loss: 15.481888771057129\n",
            "Epoch 8/100, Loss: 12.163918495178223\n",
            "Epoch 9/100, Loss: 1.446319341659546\n",
            "Epoch 10/100, Loss: 0.8458327651023865\n",
            "Epoch 11/100, Loss: 0.35092005133628845\n",
            "Epoch 12/100, Loss: 4.9857940673828125\n",
            "Epoch 13/100, Loss: 1.6962560415267944\n",
            "Epoch 14/100, Loss: 2.904329538345337\n",
            "Epoch 15/100, Loss: 0.7084870934486389\n",
            "Epoch 16/100, Loss: 0.8219170570373535\n",
            "Epoch 17/100, Loss: 0.0005193602992221713\n",
            "Epoch 18/100, Loss: 0.1495336890220642\n",
            "Epoch 19/100, Loss: 1.0776991844177246\n",
            "Epoch 20/100, Loss: 0.10086991637945175\n",
            "Epoch 21/100, Loss: 0.042469341307878494\n",
            "Epoch 22/100, Loss: 0.017738601192831993\n",
            "Epoch 23/100, Loss: 0.05337188020348549\n",
            "Epoch 24/100, Loss: 0.007501512300223112\n",
            "Epoch 25/100, Loss: 0.022048557177186012\n",
            "Epoch 26/100, Loss: 0.0011361392680555582\n",
            "Epoch 27/100, Loss: 0.0015330772148445249\n",
            "Epoch 28/100, Loss: 7.47370213503018e-05\n",
            "Epoch 29/100, Loss: 1.7251454664801713e-06\n",
            "Epoch 30/100, Loss: 0.011564982123672962\n",
            "Epoch 31/100, Loss: 0.003548906184732914\n",
            "Epoch 32/100, Loss: 1.2779539247276261e-05\n",
            "Epoch 33/100, Loss: 0.0023190255742520094\n",
            "Epoch 34/100, Loss: 0.00028143703821115196\n",
            "Epoch 35/100, Loss: 0.005218530539423227\n",
            "Epoch 36/100, Loss: 0.0029497663490474224\n",
            "Epoch 37/100, Loss: 0.00773297855630517\n",
            "Epoch 38/100, Loss: 0.000304730492644012\n",
            "Epoch 39/100, Loss: 0.00017207642667926848\n",
            "Epoch 40/100, Loss: 0.0012620484922081232\n",
            "Epoch 41/100, Loss: 4.695830284617841e-06\n",
            "Epoch 42/100, Loss: 0.0006202217191457748\n",
            "Epoch 43/100, Loss: 0.0007057285401970148\n",
            "Epoch 44/100, Loss: 6.67470958433114e-05\n",
            "Epoch 45/100, Loss: 3.202275183866732e-05\n",
            "Epoch 46/100, Loss: 2.4063559976639226e-05\n",
            "Epoch 47/100, Loss: 3.247224594815634e-05\n",
            "Epoch 48/100, Loss: 0.0001437503524357453\n",
            "Epoch 49/100, Loss: 1.0519944225961808e-05\n",
            "Epoch 50/100, Loss: 6.971279162826249e-07\n",
            "Epoch 51/100, Loss: 2.244951974716969e-05\n",
            "Epoch 52/100, Loss: 3.515861681080423e-05\n",
            "Epoch 53/100, Loss: 8.963548339124827e-07\n",
            "Epoch 54/100, Loss: 8.212510351768287e-07\n",
            "Epoch 55/100, Loss: 1.1490983524709009e-05\n",
            "Epoch 56/100, Loss: 9.515124475001357e-06\n",
            "Epoch 57/100, Loss: 6.494019544334151e-09\n",
            "Epoch 58/100, Loss: 3.1605873118678574e-07\n",
            "Epoch 59/100, Loss: 9.615407179808244e-06\n",
            "Epoch 60/100, Loss: 5.088659236207604e-07\n",
            "Epoch 61/100, Loss: 1.0298413144482765e-07\n",
            "Epoch 62/100, Loss: 8.122015060507692e-09\n",
            "Epoch 63/100, Loss: 1.63308868650347e-08\n",
            "Epoch 64/100, Loss: 3.766422196349595e-08\n",
            "Epoch 65/100, Loss: 8.185452315956354e-08\n",
            "Epoch 66/100, Loss: 3.757173772100941e-08\n",
            "Epoch 67/100, Loss: 7.439302862621844e-08\n",
            "Epoch 68/100, Loss: 7.72818111727247e-08\n",
            "Epoch 69/100, Loss: 6.124560059106443e-08\n",
            "Epoch 70/100, Loss: 6.341224434436299e-09\n",
            "Epoch 71/100, Loss: 2.4090297756629298e-08\n",
            "Epoch 72/100, Loss: 5.533365765586495e-09\n",
            "Epoch 73/100, Loss: 2.3137772586778738e-08\n",
            "Epoch 74/100, Loss: 2.4686016786290566e-08\n",
            "Epoch 75/100, Loss: 2.5547706172801554e-09\n",
            "Epoch 76/100, Loss: 1.4917986845830455e-09\n",
            "Epoch 77/100, Loss: 2.6284396881237626e-10\n",
            "Epoch 78/100, Loss: 2.2737367544323206e-13\n",
            "Epoch 79/100, Loss: 1.548892214486841e-08\n",
            "Epoch 80/100, Loss: 2.5067947717616335e-11\n",
            "Epoch 81/100, Loss: 4.707203515863512e-10\n",
            "Epoch 82/100, Loss: 1.4210854715202004e-10\n",
            "Epoch 83/100, Loss: 6.190248313941993e-11\n",
            "Epoch 84/100, Loss: 1.9122126104775816e-10\n",
            "Epoch 85/100, Loss: 6.87805368215777e-12\n",
            "Epoch 86/100, Loss: 3.384229785297066e-09\n",
            "Epoch 87/100, Loss: 6.963318810448982e-11\n",
            "Epoch 88/100, Loss: 1.5967316358000971e-10\n",
            "Epoch 89/100, Loss: 2.751221472863108e-11\n",
            "Epoch 90/100, Loss: 5.820766091346741e-11\n",
            "Epoch 91/100, Loss: 2.25611529458547e-10\n",
            "Epoch 92/100, Loss: 2.3283064365386963e-10\n",
            "Epoch 93/100, Loss: 1.1004885891452432e-10\n",
            "Epoch 94/100, Loss: 1.1004885891452432e-10\n",
            "Epoch 95/100, Loss: 2.2737367544323206e-11\n",
            "Epoch 96/100, Loss: 5.820766091346741e-11\n",
            "Epoch 97/100, Loss: 9.094947017729282e-13\n",
            "Epoch 98/100, Loss: 2.7853275241795927e-12\n",
            "Epoch 99/100, Loss: 9.094947017729282e-13\n",
            "Epoch 100/100, Loss: 0.0\n",
            "Prediction for User 2 on Item 103: 3.455143690109253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VV6XbfdmOM13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}