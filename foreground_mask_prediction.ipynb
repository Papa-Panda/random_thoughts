{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/random_thoughts/blob/main/foreground_mask_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMA15BQ0AftD",
        "outputId": "23d904b2-4a37-4861-a5ab-e6d222246cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# test reading a file, successful\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# df0=pd.read_csv('drive/My Drive/nimbus/Data_tabs_by_date.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YCtm5LvRFxUX"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnI_88GkBaaR",
        "outputId": "4ca8ad63-2b71-4192-d538-8475b98909d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Format: JPEG\n",
            "Image Mode: L\n",
            "Image Size: (790, 687)\n"
          ]
        }
      ],
      "source": [
        "# test opening a image file, successful\n",
        "path_to_image_1 = 'drive/My Drive/foreground_prediction/fg/img84.jpg'\n",
        "path_to_image_1 = 'drive/My Drive/foreground_prediction/fg/mask84.jpg'\n",
        "\n",
        "\n",
        "img = Image.open(path_to_image_1)\n",
        "\n",
        "# Display basic information about the image\n",
        "print(\"Image Format:\", img.format)\n",
        "print(\"Image Mode:\", img.mode)\n",
        "print(\"Image Size:\", img.size)\n",
        "# display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hCvulj3nEDwW"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Resize, ToPILImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4NSkC7wRAi8S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torch.nn.functional import relu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3tbxL9Lk93n9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # class UNet(nn.Module):\n",
        "# #     def __init__(self):\n",
        "# #         super(UNet, self).__init__()\n",
        "\n",
        "# #         # Down-sampling path\n",
        "# #         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "# #         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# #         # Up-sampling path\n",
        "# #         self.upconv1 = nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2)\n",
        "\n",
        "# #     def forward(self, x):\n",
        "# #         # Down-sampling\n",
        "# #         x = F.relu(self.conv1(x))\n",
        "# #         x_down = self.maxpool(x)\n",
        "\n",
        "# #         # Up-sampling\n",
        "# #         x_up = self.upconv1(x_down)\n",
        "\n",
        "# #         return x_up\n",
        "\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#         # Down-sampling path\n",
        "#         self.down_conv1 = self.conv_block(3, 64)\n",
        "#         self.down_conv2 = self.conv_block(64, 128)\n",
        "#         self.down_conv3 = self.conv_block(128, 256)\n",
        "#         self.down_conv4 = self.conv_block(256, 512)\n",
        "\n",
        "#         # Up-sampling path\n",
        "#         self.up_conv4 = self.conv_block(512, 256)\n",
        "#         self.up_conv3 = self.conv_block(256, 128)\n",
        "#         self.up_conv2 = self.conv_block(128, 64)\n",
        "\n",
        "#         # Final convolution\n",
        "#         self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "#     def conv_block(self, in_channels, out_channels):\n",
        "#         return nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Down-sampling\n",
        "#         down1 = self.down_conv1(x)\n",
        "#         down1_pool = F.max_pool2d(down1, kernel_size=2, stride=2)\n",
        "#         down2 = self.down_conv2(down1_pool)\n",
        "#         down2_pool = F.max_pool2d(down2, kernel_size=2, stride=2)\n",
        "#         down3 = self.down_conv3(down2_pool)\n",
        "#         down3_pool = F.max_pool2d(down3, kernel_size=2, stride=2)\n",
        "#         down4 = self.down_conv4(down3_pool)\n",
        "#         down4_pool = F.max_pool2d(down4, kernel_size=2, stride=2)\n",
        "\n",
        "#         # Up-sampling\n",
        "#         up4 = F.interpolate(down4_pool, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "#         print('down4', down4.shape)\n",
        "#         print('up4', up4.shape)\n",
        "#         up4 = torch.cat([down4, up4], dim=1)\n",
        "#         up4 = self.up_conv4(up4)\n",
        "#         up3 = F.interpolate(up4, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "#         up3 = torch.cat([down2, up3], dim=1)\n",
        "#         up3 = self.up_conv3(up3)\n",
        "#         up2 = F.interpolate(up3, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "#         up2 = torch.cat([down1, up2], dim=1)\n",
        "#         up2 = self.up_conv2(up2)\n",
        "\n",
        "#         # Final convolution\n",
        "#         output = self.final_conv(up2)\n",
        "#         output = torch.sigmoid(output)  # Apply sigmoid activation for binary classification\n",
        "\n",
        "#         return output\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image.\n",
        "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
        "        # -------\n",
        "        # input: 572x572x3\n",
        "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
        "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
        "\n",
        "        # input: 284x284x64\n",
        "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
        "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
        "\n",
        "        # input: 140x140x128\n",
        "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
        "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
        "\n",
        "        # input: 68x68x256\n",
        "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
        "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
        "\n",
        "        # input: 32x32x512\n",
        "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
        "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
        "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Output layer\n",
        "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        xe11 = relu(self.e11(x))\n",
        "        xe12 = relu(self.e12(xe11))\n",
        "        xp1 = self.pool1(xe12)\n",
        "\n",
        "        xe21 = relu(self.e21(xp1))\n",
        "        xe22 = relu(self.e22(xe21))\n",
        "        xp2 = self.pool2(xe22)\n",
        "\n",
        "        xe31 = relu(self.e31(xp2))\n",
        "        xe32 = relu(self.e32(xe31))\n",
        "        xp3 = self.pool3(xe32)\n",
        "\n",
        "        xe41 = relu(self.e41(xp3))\n",
        "        xe42 = relu(self.e42(xe41))\n",
        "        xp4 = self.pool4(xe42)\n",
        "\n",
        "        xe51 = relu(self.e51(xp4))\n",
        "        xe52 = relu(self.e52(xe51))\n",
        "\n",
        "        # Decoder\n",
        "        xu1 = self.upconv1(xe52)\n",
        "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
        "        xd11 = relu(self.d11(xu11))\n",
        "        xd12 = relu(self.d12(xd11))\n",
        "\n",
        "        xu2 = self.upconv2(xd12)\n",
        "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
        "        xd21 = relu(self.d21(xu22))\n",
        "        xd22 = relu(self.d22(xd21))\n",
        "\n",
        "        xu3 = self.upconv3(xd22)\n",
        "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
        "        xd31 = relu(self.d31(xu33))\n",
        "        xd32 = relu(self.d32(xd31))\n",
        "\n",
        "        xu4 = self.upconv4(xd32)\n",
        "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
        "        xd41 = relu(self.d41(xu44))\n",
        "        xd42 = relu(self.d42(xd41))\n",
        "\n",
        "        # Output layer\n",
        "        out = self.outconv(xd42)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "eyCgezClAI4C"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    # def __init__(self, image_paths, mask_paths, transform=None, target_size=(64, 64)):\n",
        "    def __init__(self, image_paths, mask_paths, transform=None, target_size=(256, 256)):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx])\n",
        "        mask = Image.open(self.mask_paths[idx])\n",
        "        # Resize images and masks to the target size\n",
        "        image = Resize(self.target_size)(image)\n",
        "        mask = Resize(self.target_size, interpolation=Image.NEAREST)(mask)\n",
        "        # display(image)\n",
        "        # display(mask)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        smooth = 1e-6\n",
        "        intersection = (inputs * targets).sum()\n",
        "        union = inputs.sum() + targets.sum()\n",
        "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
        "        loss = 1 - dice\n",
        "        return loss\n",
        "\n",
        "# Example paths to images and masks\n",
        "# image_paths = [\"path_to_image_1.jpg\", \"path_to_image_2.jpg\", ...]\n",
        "image_paths = [ 'drive/My Drive/foreground_prediction/fg/img'+str(i)+'.jpg' for i in range(292)]\n",
        "# mask_paths = [\"path_to_mask_1.jpg\", \"path_to_mask_2.jpg\", ...]\n",
        "mask_paths = [ 'drive/My Drive/foreground_prediction/fg/mask'+str(i)+'.jpg' for i in range(292)]\n",
        "\n",
        "# Create custom dataset\n",
        "dataset = CustomDataset(image_paths, mask_paths, transform=ToTensor())\n",
        "\n",
        "# Define data loader\n",
        "batch_size = 4\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = UNet()\n",
        "# criterion = nn.BCELoss()\n",
        "# criterion = DiceLoss()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRXoe0-CCq9s",
        "outputId": "48daae4b-3cb9-4b47-e559-4cf40846d5d5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 73/73 [22:40<00:00, 18.64s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.08967735467810337\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 73/73 [20:47<00:00, 17.09s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Loss: 0.05127813797188948\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 73/73 [20:46<00:00, 17.07s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10, Loss: 0.04315163404361842\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 73/73 [20:46<00:00, 17.07s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Loss: 0.047117446702330895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10:  44%|████▍     | 32/73 [09:07<11:40, 17.09s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, masks in tqdm(data_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate loss\n",
        "        # print(outputs)\n",
        "        # print(masks)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'unet_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1u0vCi5A90r"
      },
      "outputs": [],
      "source": [
        "# image size input are slighlty different\n",
        "# need to normalize\n",
        "# first try cutting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV8A9gbnDSEw"
      },
      "outputs": [],
      "source": [
        "# # image_paths = [ 'drive/My Drive/foreground_prediction/fg/img'+str(i)+'.jpg' for i in range(292)]\n",
        "# # # mask_paths = [\"path_to_mask_1.jpg\", \"path_to_mask_2.jpg\", ...]\n",
        "# # mask_paths = [ 'drive/My Drive/foreground_prediction/fg/mask'+str(i)+'.jpg' for i in range(292)]\n",
        "# # path_to_image_1 = 'drive/My Drive/foreground_prediction/fg/img84.jpg'\n",
        "\n",
        "# for i in range(20):\n",
        "\n",
        "#   img = Image.open(image_paths[i])\n",
        "#   print(\"Image Size:\", img.size)\n",
        "#   img = Image.open(mask_paths[i])\n",
        "#   print(\"Image Size:\", img.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9pLsSC9HALs"
      },
      "outputs": [],
      "source": [
        "# double checked the data by display(image), the image is read well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb5r1ONIDj49"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = UNet()\n",
        "model.load_state_dict(torch.load('unet_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define the transform to convert PIL image to tensor\n",
        "transform = ToTensor()\n",
        "\n",
        "# Load and preprocess the image\n",
        "pick_index = 90\n",
        "image_path = image_paths[pick_index]\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor)\n",
        "\n",
        "# Post-process the output\n",
        "output_binary = (output > 0.5).float()  # Convert probabilities to binary values (0 or 1)\n",
        "\n",
        "# Convert the tensor back to PIL image\n",
        "output_image = ToPILImage()(output_binary.squeeze(0))\n",
        "\n",
        "display(image)\n",
        "display( output_image )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54X-Dihnp5qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG7dZUNvEQWBLITJ0/QWMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}